import gym
import numpy as np
import os
from stable_baselines3 import PPO, DQN, A2C
from trading.binance_futures import execute_trade
from notifications.telegram_bot import send_telegram_message
from gym.envs.registration import register

# ğŸ“Œ **Ã–zel Ticaret OrtamÄ±nÄ± Kaydet ve TanÄ±mla**
try:
    register(
        id="TradingEnv-v0",
        entry_point="trading_env:TradingEnv",
    )
except:
    print("âš ï¸ TradingEnv zaten kayÄ±tlÄ±!")

# ğŸ“Œ **OrtamÄ± OluÅŸtur**
try:
    env = gym.make("TradingEnv-v0")
except Exception as e:
    print(f"âš ï¸ Ortam baÅŸlatÄ±lamadÄ±: {e}")
    env = None

# ğŸ“Œ **Model DosyalarÄ±nÄ± Kaydetme ve YÃ¼kleme Ä°ÅŸlemi**
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

def load_or_create_model(model_class, model_name):
    """KaydedilmiÅŸ modeli yÃ¼kler veya yeni model oluÅŸturur."""
    model_path = os.path.join(MODEL_DIR, model_name)
    if os.path.exists(model_path + ".zip"):
        print(f"âœ… {model_name} modeli yÃ¼kleniyor...")
        return model_class.load(model_path, env=env)
    else:
        print(f"ğŸ”„ {model_name} modeli oluÅŸturuluyor...")
        return model_class("MlpPolicy", env, verbose=1)

# ğŸ“Œ **Modelleri TanÄ±mla**
ppo_model = load_or_create_model(PPO, "ppo_trading_model")
dqn_model = load_or_create_model(DQN, "dqn_trading_model")
a2c_model = load_or_create_model(A2C, "a2c_trading_model")

def train_models(total_timesteps=100000):
    """ğŸ“ˆ PPO, DQN ve A2C modellerini eÄŸit ve kaydet"""
    if env is None:
        print("âš ï¸ Ortam baÅŸlatÄ±lamadÄ±! Model eÄŸitimi gerÃ§ekleÅŸtirilemiyor.")
        return
    
    print("ğŸ”„ PPO Modeli EÄŸitiliyor...")
    ppo_model.learn(total_timesteps=total_timesteps)
    ppo_model.save(os.path.join(MODEL_DIR, "ppo_trading_model"))

    print("ğŸ”„ DQN Modeli EÄŸitiliyor...")
    dqn_model.learn(total_timesteps=total_timesteps)
    dqn_model.save(os.path.join(MODEL_DIR, "dqn_trading_model"))

    print("ğŸ”„ A2C Modeli EÄŸitiliyor...")
    a2c_model.learn(total_timesteps=total_timesteps)
    a2c_model.save(os.path.join(MODEL_DIR, "a2c_trading_model"))

    print("âœ… Reinforcement Learning Modelleri EÄŸitildi ve Kaydedildi!")

# ğŸ“Œ **PPO Modeli ile Ä°ÅŸlem AÃ§**
def reinforcement_trade_ppo():
    """ğŸ“Š PPO modeli ile AI destekli iÅŸlem aÃ§ar"""
    if env is None:
        print("âš ï¸ Ortam baÅŸlatÄ±lamadÄ±! Ä°ÅŸlem aÃ§Ä±lamÄ±yor.")
        return

    obs = env.reset()
    action, _ = ppo_model.predict(obs)
    trade_type = "LONG" if action == 1 else "SHORT"
    execute_trade(symbol="BTCUSDT", trade_type=trade_type, quantity=0.01, leverage=5)
    send_telegram_message(f"ğŸ“ˆ PPO Modeli KararÄ±: {trade_type}")

# ğŸ“Œ **DQN Modeli ile Ä°ÅŸlem AÃ§**
def reinforcement_trade_dqn():
    """ğŸ“Š DQN modeli ile AI destekli iÅŸlem aÃ§ar"""
    if env is None:
        print("âš ï¸ Ortam baÅŸlatÄ±lamadÄ±! Ä°ÅŸlem aÃ§Ä±lamÄ±yor.")
        return

    obs = env.reset()
    action, _ = dqn_model.predict(obs)
    trade_type = "LONG" if action == 1 else "SHORT"
    execute_trade(symbol="BTCUSDT", trade_type=trade_type, quantity=0.01, leverage=5)
    send_telegram_message(f"ğŸ“ˆ DQN Modeli KararÄ±: {trade_type}")

# ğŸ“Œ **A2C Modeli ile Ä°ÅŸlem AÃ§**
def reinforcement_trade_a2c():
    """ğŸ“Š A2C modeli ile AI destekli iÅŸlem aÃ§ar"""
    if env is None:
        print("âš ï¸ Ortam baÅŸlatÄ±lamadÄ±! Ä°ÅŸlem aÃ§Ä±lamÄ±yor.")
        return

    obs = env.reset()
    action, _ = a2c_model.predict(obs)
    trade_type = "LONG" if action == 1 else "SHORT"
    execute_trade(symbol="BTCUSDT", trade_type=trade_type, quantity=0.01, leverage=5)
    send_telegram_message(f"ğŸ“ˆ A2C Modeli KararÄ±: {trade_type}")

# ğŸ“Œ **EÄŸer bu dosya doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rsa modeller eÄŸitilir**
if __name__ == "__main__":
    train_models(total_timesteps=100000)
